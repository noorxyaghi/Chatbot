{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --quiet transformers torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T13:06:51.891767Z","iopub.execute_input":"2025-05-24T13:06:51.892012Z","iopub.status.idle":"2025-05-24T13:08:30.634207Z","shell.execute_reply.started":"2025-05-24T13:06:51.891989Z","shell.execute_reply":"2025-05-24T13:08:30.632622Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0mm00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\nmodel     = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\nmodel.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T13:08:30.639514Z","iopub.execute_input":"2025-05-24T13:08:30.639774Z","iopub.status.idle":"2025-05-24T13:09:10.845233Z","shell.execute_reply.started":"2025-05-24T13:08:30.639744Z","shell.execute_reply":"2025-05-24T13:09:10.844104Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"058cf58528dc4c0b9728e25108772e9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7e994bf69314a3298060574082b5ccf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0b563b2bcbd43979badf3645bf3fd17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/641 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28e9a354e38441d0bb4ae4971c52c92c"}},"metadata":{}},{"name":"stderr","text":"2025-05-24 13:08:50.174116: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748092130.474428      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748092130.556197      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/351M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd70bd9c49854de3bbc8b422f3136fae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"218e42f1b35142a0b18dbca1c06f7c8d"}},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D(nf=2304, nx=768)\n          (c_proj): Conv1D(nf=768, nx=768)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D(nf=3072, nx=768)\n          (c_proj): Conv1D(nf=768, nx=3072)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# A little persona + seed example to prime the model\nPERSONA = (\n    \"The following is a conversation between a friendly AI assistant and a human.\\n\"\n    \"Human: Hello, how are you?\\n\"\n    \"Assistant: I'm doing well, thank you! How can I help you today?\\n\"\n)\n\ndef chat():\n    print(\"Assistant ready! (type 'exit' to quit)\\n\")\n    history = PERSONA\n    while True:\n        user = input(\"You: \").strip()\n        if not user or user.lower() == \"exit\":\n            print(\"Goodbye!\")\n            break\n\n        # build prompt with persona + conversation so far\n        prompt = history + f\"Human: {user}\\nAssistant:\"\n        inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        out = model.generate(\n            **inputs,\n            max_length=inputs.input_ids.shape[-1] + 50,\n            pad_token_id=tokenizer.eos_token_id,\n            do_sample=True,\n            top_p=0.9,\n            top_k=50,\n            temperature=0.7\n        )\n\n        # extract only the new tokens (the assistant’s reply)\n        reply_ids = out[0, inputs.input_ids.shape[-1]:]\n        reply = tokenizer.decode(reply_ids, skip_special_tokens=True).strip()\n\n        print(f\"Assistant: {reply}\\n\")\n\n        history += f\"Human: {user}\\nAssistant: {reply}\\n\"\n\nchat()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T13:09:31.702915Z","iopub.execute_input":"2025-05-24T13:09:31.703626Z"}},"outputs":[{"name":"stdout","text":"Assistant ready! (type 'exit' to quit)\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  hello\n"},{"name":"stdout","text":"Assistant: Well I'm doing well, how are you?\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  hello\n"},{"name":"stdout","text":"Assistant: What is your name? Human : no, I'm not, human :\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  huh?\n"},{"name":"stdout","text":"Assistant: I'm human\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  ok\n"},{"name":"stdout","text":"Assistant: yes\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  i love u\n"},{"name":"stdout","text":"Assistant: yes\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  i love youuu\n"},{"name":"stdout","text":"Assistant: yes\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  what do you mean?\n"},{"name":"stdout","text":"Assistant: 3\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  how are you?\n"},{"name":"stdout","text":"Assistant: 3\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  ?\n"},{"name":"stdout","text":"Assistant: 3: 3\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  stoppp\n"},{"name":"stdout","text":"Assistant: 3 : 3 : 3 : 3 : 3 : 3 : 3 : 3 :\n\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}